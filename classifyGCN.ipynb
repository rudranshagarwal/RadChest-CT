{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import KarateClub\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader as dataloader_normal, TensorDataset\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import nibabel as nib\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from skimage.measure import regionprops\n",
    "from torch_geometric.nn import GCNConv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def get_edges(self,centroids):\n",
    "        \n",
    "\n",
    "        k_neighbors = self.k\n",
    "        knn_model = NearestNeighbors(n_neighbors=k_neighbors)\n",
    "\n",
    "        knn_model.fit(centroids)\n",
    "\n",
    "        distances, indices = knn_model.kneighbors(centroids)\n",
    "\n",
    "        indices = indices[:, :]\n",
    "        edges = [[],[]]\n",
    "        for i in range(420):\n",
    "            for j in range(4):\n",
    "                edges[0].append(i)\n",
    "                edges[1].append(indices[i][j])\n",
    "        return edges\n",
    "    def __init__(self, annotations_file, feature_dir, slic_dir, lungmask_dir,centroids_dir, k):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = feature_dir\n",
    "        self.feature_paths = []\n",
    "        self.labels = []\n",
    "        self.slic_paths = []\n",
    "        self.lungmask_paths = []\n",
    "        self.k = k\n",
    "        self.centroid_paths = []\n",
    "        problem = [\"/scratch/features/train/set22/trn24016.pth\",\n",
    "        \"/scratch/features/train/set12/trn12411.pth\",\n",
    "        \"/scratch/features/train/set9/trn09320.pth\",\n",
    "        \"/scratch/features/train/set18/trn19397.pth\",\n",
    "        \"/scratch/features/train/set7/trn07778.pth\",\n",
    "        \"/scratch/features/train/set14/trn15124.pth\",\n",
    "        \"/scratch/features/train/set14/trn15109.pth\",\n",
    "        \"/scratch/features/train/set10/trn10310.pth\",\n",
    "        \"/scratch/features/train/set13/trn14334.pth\"]\n",
    "        for set in os.listdir(feature_dir):\n",
    "            set_path = os.path.join(feature_dir, set)\n",
    "            set_path_slic = os.path.join(slic_dir,set)\n",
    "            set_path_lungmask = os.path.join(lungmask_dir, set)\n",
    "            set_path_centroids = os.path.join(centroids_dir, set)\n",
    "            for feature in os.listdir(set_path):\n",
    "                image = feature.split(\".\")[0]\n",
    "                feature_path = os.path.join(set_path, feature)\n",
    "                slic_path = os.path.join(set_path_slic, image + '.nii')\n",
    "                lungmask_path = os.path.join(set_path_lungmask, image + '.nii')\n",
    "                centroids_path = os.path.join(set_path_centroids, image + '.npy')\n",
    "                if(feature_path in problem):\n",
    "                    continue\n",
    "                self.feature_paths.append(feature_path)\n",
    "                self.slic_paths.append(slic_path)\n",
    "                self.lungmask_paths.append(lungmask_path)\n",
    "                self.centroid_paths.append(centroids_path)\n",
    "                self.labels.append(torch.tensor(self.img_labels[self.img_labels['NoteAcc_DEID'] == image][['nodule*lung', 'opacity*lung', 'atelectasis*lung', 'consolidation*lung', 'mass*lung', 'pneumothorax*lung']].values[0]))\n",
    "                # self.labels.append(torch.tensor(self.img_labels[self.img_labels['NoteAcc_DEID'] == image][['opacity*lung']].values[0]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = torch.load(self.feature_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # slic = nib.load(self.slic_paths[idx]).get_fdata()\n",
    "        \n",
    "        centroids = np.load(self.centroid_paths[idx])\n",
    "        \n",
    "        # lungmask = nib.load(self.lungmask_paths[idx]).get_fdata()\n",
    "        edges = self.get_edges(centroids)\n",
    "        \n",
    "        return feature, label, torch.tensor(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file = 'imgtrain_Abnormality_and_Location_Labels.csv'\n",
    "train_features = '/scratch/features/train'\n",
    "train_slic = '/home/RadChest/slic/train'\n",
    "train_lungmask = '/home/RadChest/lungmask/train'\n",
    "train_centroid = '/scratch/centroids/train'\n",
    "traindata = CustomDataset(label_file, train_features, train_slic, train_lungmask,train_centroid, 20)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=2048\n",
    "num_labels=1\n",
    "class MultiLabelNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MultiLabelNN, self).__init__()\n",
    "        self.conv1 = GCNConv(2048,1024)\n",
    "        self.conv2 = GCNConv(1024,512)\n",
    "        self.conv3 = GCNConv(512,128)\n",
    "        self.conv4 = GCNConv(128, 16)\n",
    "        self.fc1 = nn.Linear(16*420, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 64)\n",
    "        self.fc4 = nn.Linear(64, output_dim)\n",
    "    \n",
    "    def forward(self, data): \n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.conv4(x, edge_index).relu()\n",
    "        x = x.reshape((-1,420*16))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = MultiLabelNN(num_features, num_labels).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()  # For multi-label classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorch2/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([512, 6])) must be the same as input size (torch.Size([512, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# outputs = model(inputs, edges)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batchx)\n\u001b[0;32m---> 25\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.10/site-packages/torch/nn/modules/loss.py:725\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.10/site-packages/torch/nn/functional.py:3193\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3190\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([512, 6])) must be the same as input size (torch.Size([512, 1]))"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "dataloader = dataloader_normal(traindata, batch_size=512, shuffle=True, num_workers=30)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0\n",
    "    epoch_outputs = torch.empty((0,1))\n",
    "    epoch_targets = torch.empty((0,1))\n",
    "    i = 0\n",
    "    for inputs, targets, edges in dataloader:\n",
    "        print(i)\n",
    "        i+=1\n",
    "        graphs = [Data(x = inputs[j], edge_index = edges[j]) for j in range(inputs.shape[0])]\n",
    "        dataloader_graph = DataLoader(graphs, batch_size=inputs.shape[0],num_workers=30)\n",
    "        batchx = []\n",
    "        for databatch in dataloader_graph:\n",
    "            batchx = databatch\n",
    "        # inputs = inputs.to(device)\n",
    "        batchx = batchx.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # edges = edges.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # outputs = model(inputs, edges)\n",
    "        outputs = model(batchx)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_outputs = torch.cat((epoch_outputs, outputs.cpu()))\n",
    "        epoch_targets = torch.cat((epoch_targets, targets.cpu()))\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(traindata)\n",
    "    probabilities = torch.sigmoid(epoch_outputs)\n",
    "    predictions = (probabilities > 0.5).float()\n",
    "\n",
    "    predictions_np = predictions.detach().numpy()\n",
    "    targets_np = epoch_targets.numpy()\n",
    "\n",
    "    average_precision = average_precision_score(targets_np, predictions_np, average='macro')\n",
    "    auroc_score = roc_auc_score(targets_np,predictions_np,average='macro')\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Average Precision (Macro): {average_precision:.4f}, AUROC: {auroc_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions_np, targets_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(targets_np[j][i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     12\u001b[0m         c\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(c, \u001b[43ma\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mb\u001b[49m)\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m/\u001b[39mb\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "for i in range(6):\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    for j in range(predictions_np.shape[0]):\n",
    "        if(predictions_np[j][i] == 1):\n",
    "            if(predictions_np[j][i] == targets_np[j][i]):\n",
    "                a += 1\n",
    "            b += 1\n",
    "        if(targets_np[j][i] == 1):\n",
    "            c+= 1\n",
    "    print(c, a/b)\n",
    "    x += a/b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[88620, 2048], edge_index=[2, 354480], batch=[88620], ptr=[212])\n"
     ]
    }
   ],
   "source": [
    "print(batchx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorch2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 211 worker processes in total. Our suggested max number of worker in current system is 48, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[88620, 2048], edge_index=[2, 354480], batch=[88620], ptr=[212])\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(dataloader_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[88620, 2048], edge_index=[2, 354480], batch=[88620], ptr=[212])\n"
     ]
    }
   ],
   "source": [
    "print(batchx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680]), Data(x=[420, 2048], edge_index=[2, 1680])]\n"
     ]
    }
   ],
   "source": [
    "print(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([420, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbatch\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MultiLabelNN.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m edges \u001b[38;5;241m=\u001b[39m edges\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: MultiLabelNN.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for inputs, targets, edges in traindata:\n",
    "    model.train()  \n",
    "    epoch_outputs = torch.empty((0,6))\n",
    "    epoch_targets = torch.empty((0,6))\n",
    "    print(i)\n",
    "    i+=1\n",
    "    if(i == 2):\n",
    "        break\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device).reshape((1,6))\n",
    "        edges = edges.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, edges)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_outputs = torch.cat((epoch_outputs, outputs.cpu()))\n",
    "        epoch_targets = torch.cat((epoch_targets, targets.cpu()))\n",
    "        epoch_loss = loss.item() \n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predictions = (probabilities > 0.5).float()\n",
    "        print(f'epoch: {epoch}: {epoch_loss:.4f} Predictions: {predictions} target: {targets}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1.bias | Size: torch.Size([512]) | Values : tensor([0.0052, 0.0054], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv1.lin.weight | Size: torch.Size([512, 2048]) | Values : tensor([[-0.0233, -0.0419, -0.0058,  ...,  0.0100,  0.0238,  0.0218],\n",
      "        [-0.0005,  0.0066, -0.0137,  ...,  0.0385,  0.0023, -0.0347]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.bias | Size: torch.Size([16]) | Values : tensor([-0.0060, -0.0067], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: conv2.lin.weight | Size: torch.Size([16, 512]) | Values : tensor([[-0.1120,  0.0025, -0.0936,  ...,  0.0110,  0.0789, -0.0176],\n",
      "        [ 0.0371, -0.0806,  0.0841,  ..., -0.0558, -0.0736, -0.0106]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc1.weight | Size: torch.Size([128, 6720]) | Values : tensor([[-0.0090,  0.0042,  0.0097,  ...,  0.0067,  0.0033, -0.0159],\n",
      "        [-0.0165, -0.0073, -0.0012,  ...,  0.0085, -0.0061, -0.0070]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc1.bias | Size: torch.Size([128]) | Values : tensor([0.0302, 0.0194], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc2.weight | Size: torch.Size([64, 128]) | Values : tensor([[-0.0382, -0.0119,  0.0453, -0.0444,  0.0761, -0.0222,  0.0594,  0.0594,\n",
      "         -0.0499, -0.0393, -0.0865, -0.0136,  0.0660, -0.0075, -0.0255,  0.0425,\n",
      "         -0.0566, -0.0656, -0.0156, -0.0598,  0.0395, -0.0461,  0.0190,  0.0678,\n",
      "         -0.0858,  0.0599,  0.0552,  0.0340, -0.0587, -0.0859,  0.0557, -0.0233,\n",
      "          0.0391,  0.0126,  0.0418,  0.0709,  0.0717, -0.0013,  0.0397,  0.0084,\n",
      "         -0.0295, -0.0062,  0.0869,  0.0484, -0.0651, -0.0864, -0.0627,  0.0815,\n",
      "         -0.0401,  0.0648,  0.0886, -0.0256,  0.0196,  0.0130,  0.0591, -0.0874,\n",
      "          0.0627, -0.0633,  0.0711, -0.0666,  0.0489,  0.0002, -0.0557, -0.0506,\n",
      "         -0.0757, -0.0269, -0.0822,  0.0225,  0.0012,  0.0482, -0.0121,  0.0306,\n",
      "         -0.0756, -0.0587, -0.0650, -0.0180,  0.0386,  0.0136, -0.0800,  0.0673,\n",
      "          0.0352,  0.0089, -0.0247,  0.0165,  0.0843,  0.0431,  0.0706, -0.0026,\n",
      "          0.0754,  0.0574,  0.0443, -0.0514, -0.0112,  0.0374,  0.0410, -0.0546,\n",
      "         -0.0153,  0.0404,  0.0470, -0.0343,  0.0825, -0.0560,  0.0045, -0.0586,\n",
      "         -0.0487, -0.0260,  0.0148,  0.0327,  0.0064,  0.0463,  0.0091,  0.0479,\n",
      "          0.0564,  0.0282, -0.0661, -0.0393,  0.0120,  0.0655, -0.0508, -0.0377,\n",
      "          0.0115, -0.0051,  0.0365, -0.0269,  0.0266,  0.0527, -0.0323, -0.0311],\n",
      "        [ 0.0509,  0.0088, -0.0298, -0.0609,  0.0628,  0.0659,  0.0818, -0.0559,\n",
      "          0.0198,  0.0666,  0.0503, -0.0797, -0.0202,  0.0683,  0.0609, -0.0930,\n",
      "          0.0242, -0.0347, -0.0123,  0.0145,  0.0778, -0.0575,  0.0713,  0.0472,\n",
      "          0.0489,  0.0333, -0.0253, -0.0131, -0.0899, -0.0204, -0.0686,  0.0307,\n",
      "         -0.0753,  0.0570,  0.0489,  0.0664,  0.0771,  0.0774, -0.0131,  0.0720,\n",
      "          0.0144,  0.0683,  0.0087,  0.0035, -0.0025, -0.0003, -0.0444, -0.0878,\n",
      "          0.0241,  0.0016, -0.0901, -0.0454, -0.0549,  0.0604, -0.0349, -0.0685,\n",
      "         -0.0038, -0.0879, -0.0831,  0.0705,  0.0318, -0.0106,  0.0677,  0.0459,\n",
      "         -0.0436, -0.0476, -0.0276, -0.0529,  0.0593, -0.0853, -0.0139, -0.0786,\n",
      "         -0.0941, -0.0235, -0.0527,  0.0604,  0.0655, -0.0466,  0.0092, -0.0500,\n",
      "         -0.0465,  0.0448, -0.0571, -0.0371,  0.0554, -0.0127,  0.0276,  0.0394,\n",
      "         -0.0007,  0.0186, -0.0830, -0.0407,  0.0606,  0.0050,  0.0714, -0.0168,\n",
      "         -0.0087, -0.0517,  0.0185, -0.0383,  0.0505,  0.0524, -0.0428,  0.0230,\n",
      "         -0.0325, -0.0154,  0.0081,  0.0162, -0.0481,  0.0321,  0.0621, -0.0079,\n",
      "          0.0654, -0.0136,  0.0667,  0.0671, -0.0815,  0.0648,  0.0388,  0.0801,\n",
      "          0.0698, -0.0023,  0.0708, -0.0388,  0.0071,  0.0421,  0.0219,  0.0304]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc2.bias | Size: torch.Size([64]) | Values : tensor([0.0892, 0.0303], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc3.weight | Size: torch.Size([1, 64]) | Values : tensor([[ 0.0103,  0.1169, -0.1069, -0.1069, -0.0372, -0.0040, -0.0316, -0.0589,\n",
      "         -0.0306, -0.1203, -0.0052, -0.1100,  0.1079,  0.1154,  0.0304, -0.0838,\n",
      "          0.0745,  0.0815, -0.0136,  0.0393, -0.0547, -0.0827, -0.1302,  0.0728,\n",
      "          0.0053, -0.1164, -0.0266,  0.0668,  0.0137,  0.0426,  0.0243, -0.0005,\n",
      "         -0.0580,  0.0195,  0.0737,  0.0439,  0.1068,  0.0034, -0.0276, -0.0642,\n",
      "          0.0267,  0.0145,  0.0519, -0.0316,  0.0464, -0.0092, -0.0708, -0.0844,\n",
      "         -0.0188, -0.0045, -0.0498,  0.0053,  0.0473,  0.0526,  0.0794, -0.1001,\n",
      "          0.0753,  0.0031,  0.0976,  0.0640,  0.1029, -0.0177,  0.0911,  0.0995]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: fc3.bias | Size: torch.Size([1]) | Values : tensor([-0.0634], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
